{"cells":[{"cell_type":"markdown","metadata":{"id":"5fNts3QHDJHd"},"source":["# Instructions\n","1. Export an OpenAI API key as an environment variable under the name \"OPENAI_API_KEY\". Without this, RAG won't run!\n","2. Click “Run All” in run options on the top bar.\n","3. In the last tab, input whatever query as the parameter for the function “query_engine.query” and then rerun the final code block to acquire another answer for the new query.\n","\n","(Note that for the sake of demonstration, we have commented out the code supporting persisting index values, because Google Colab itself cannot maintain persistent data and thus we would require a particular Google Drive folder format. Because there is a small amount of data, we believe it is an acceptable cost to just reconstruct an index.)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qx4Q1VTKnAkx","outputId":"d38a147f-e9ab-43c3-9788-75df765a0bbc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting llama-index\n","  Downloading llama_index-0.10.30-py3-none-any.whl (6.9 kB)\n","Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n","  Downloading llama_index_agent_openai-0.2.2-py3-none-any.whl (12 kB)\n","Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n","  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n","Collecting llama-index-core<0.11.0,>=0.10.30 (from llama-index)\n","  Downloading llama_index_core-0.10.30-py3-none-any.whl (15.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n","  Downloading llama_index_embeddings_openai-0.1.8-py3-none-any.whl (6.0 kB)\n","Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 (from llama-index)\n","  Downloading llama_index_indices_managed_llama_cloud-0.1.5-py3-none-any.whl (6.7 kB)\n","Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n","  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n","  Downloading llama_index_llms_openai-0.1.16-py3-none-any.whl (10 kB)\n","Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n","  Downloading llama_index_multi_modal_llms_openai-0.1.5-py3-none-any.whl (5.8 kB)\n","Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n","  Downloading llama_index_program_openai-0.1.5-py3-none-any.whl (4.1 kB)\n","Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n","  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n","Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n","  Downloading llama_index_readers_file-0.1.19-py3-none-any.whl (36 kB)\n","Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2 (from llama-index)\n","  Downloading llama_index_readers_llama_parse-0.1.4-py3-none-any.whl (2.5 kB)\n","Collecting openai>=1.14.0 (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index)\n","  Downloading openai-1.23.2-py3-none-any.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (6.0.1)\n","Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (2.0.29)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (3.9.5)\n","Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n","  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n","Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n","  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (2023.6.0)\n","Collecting httpx (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting llamaindex-py-client<0.2.0,>=0.1.18 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n","  Downloading llamaindex_py_client-0.1.18-py3-none-any.whl (136 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.1/136.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (3.3)\n","Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (1.25.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (2.0.3)\n","Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (9.4.0)\n","Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (8.2.3)\n","Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n","  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (4.66.2)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (4.11.0)\n","Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.30->llama-index)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.30->llama-index) (1.14.1)\n","Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n","Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n","  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n","  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n","Collecting llama-parse<0.5.0,>=0.4.0 (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index)\n","  Downloading llama_parse-0.4.1-py3-none-any.whl (7.3 kB)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.30->llama-index) (4.0.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n","Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.30->llama-index) (2.7.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index) (2024.2.2)\n","Collecting httpcore==1.* (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index) (3.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.3.1)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.30->llama-index)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.30->llama-index) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.4.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.30->llama-index) (2023.12.25)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.7.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.30->llama-index) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.30->llama-index) (2.0.7)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.30->llama-index) (3.0.3)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.30->llama-index)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.30->llama-index)\n","  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.30->llama-index) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.30->llama-index) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.30->llama-index) (2024.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.2.0)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.30->llama-index) (24.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.30->llama-index) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.30->llama-index) (2.18.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.30->llama-index) (1.16.0)\n","Installing collected packages: striprtf, dirtyjson, pypdf, mypy-extensions, marshmallow, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llamaindex-py-client, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n","Successfully installed dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llama-index-0.10.30 llama-index-agent-openai-0.2.2 llama-index-cli-0.1.12 llama-index-core-0.10.30 llama-index-embeddings-openai-0.1.8 llama-index-indices-managed-llama-cloud-0.1.5 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.16 llama-index-multi-modal-llms-openai-0.1.5 llama-index-program-openai-0.1.5 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.19 llama-index-readers-llama-parse-0.1.4 llama-parse-0.4.1 llamaindex-py-client-0.1.18 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.23.2 pypdf-4.2.0 striprtf-0.0.26 tiktoken-0.6.0 typing-inspect-0.9.0\n"]}],"source":["!pip install llama-index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJmizF2OpZyX"},"outputs":[],"source":["from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n","from llama_index.core import Settings\n","from llama_index.core import StorageContext, load_index_from_storage\n","from llama_index.core import PromptTemplate\n","# from google.colab import userdata, drive\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybcPoL-dA3Kh","outputId":"883ba8f5-149d-4e1f-ca99-6949ba527ae1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /mnt/drive\n"]}],"source":["# drive.mount(\"/mnt/drive\")"]},{"cell_type":"markdown","metadata":{"id":"6prCqLCH8e9C"},"source":["Mount drive in order to load in generated output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUf2739B_ifl"},"outputs":[],"source":["filelink = lambda filename : {'video_link': os.path.splitext(filename)[0]}\n","def filelink_fn(filename):\n","  basename = os.path.basename(filename)\n","  basename = basename.replace(\" \", \"/\")\n","  basename = basename.replace(\"_\", \":\")\n","  basename = basename.replace(\",\", \"?\")\n","  if (not basename.startswith(\"https\")):\n","    basename = \"\"\n","  return {\"video_link\": os.path.splitext(basename)[0], \"timestamp_format\": \"seconds\"}\n","\n","path = \"/mnt/drive/MyDrive/CSCI2270/data\"\n","documents = SimpleDirectoryReader(path, file_metadata=filelink_fn).load_data()"]},{"cell_type":"markdown","metadata":{"id":"XQUF_tZPDi-h"},"source":["Replace the Path field with the filepath to the directory containing the data on local machine, instead of the Google Drive path."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VuQriNj22URJ"},"outputs":[],"source":["# os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"]},{"cell_type":"markdown","metadata":{"id":"4-A_mnXu3TrQ"},"source":["By default, Llamaindex uses GPT 3.5 for both embeddings and LLM. Here, we load in the OpenAI API key to use GPT. Note that GPT has much stronger performance, at least compared to HuggingFace LLMs. Set the OpenAI API key as an environment variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2GssgNk9G3wA"},"outputs":[],"source":["persist_dir = \"/mnt/drive/MyDrive/CSCI2270/storage\"\n","\n","index = None\n","# if (os.path.exists(persist_dir)):\n","#   storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n","#   index = load_index_from_storage(storage_context)\n","# else:\n","index = VectorStoreIndex.from_documents(documents)\n","index.storage_context.persist(persist_dir)"]},{"cell_type":"markdown","metadata":{"id":"zDpGVaE-E3hD"},"source":["Store index into persistent data in Google Drive, and load it if possible. Index construction is *very* expensive, both time and LLM token wise, so we would like to avoid it if possible"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8qtWm0IjcmY"},"outputs":[],"source":["query_engine = index.as_query_engine(streaming=True, similarity_top_k=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":569},"id":"4lag3cUcjaCi","outputId":"77fc4519-045e-4a87-845d-9b024fe5bc34"},"outputs":[{"data":{"text/markdown":["**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Context information is below.\n","---------------------\n","{context_str}\n","---------------------\n","Given the context information and not prior knowledge, answer the query.\n","Query: {query_str}\n","Answer: \n"]},{"data":{"text/markdown":["<br><br>"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["The original query is as follows: {query_str}\n","We have provided an existing answer: {existing_answer}\n","We have the opportunity to refine the existing answer (only if needed) with some more context below.\n","------------\n","{context_msg}\n","------------\n","Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n","Refined Answer: \n"]},{"data":{"text/markdown":["<br><br>"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Context information is below.\n","---------------------\n","{context_str}\n","---------------------\n","Given the context information and not prior knowledge, answer the query.\n","Query: {query_str}\n","Answer: \n"]}],"source":["prompts_dict = query_engine.get_prompts()\n","custom_prompt = (\n","  \"Context information is below.\\n\"\n","  \"---------------------\\n\"\n","  \"{context_str}\\n\"\n","  \"---------------------\\n\"\n","  \"Given the context information and not prior knowledge, answer the query.\\n\"\n","  \"Some rules to follow:\\n\"\n","  \"1. If the query asks for a recipe or a dish and the recipe has an associated video link, please include the relevant video link.\\n\"\n","  \"2. If the query asks for a recipe, please write down the recipe as a numbered list with start and end timestamps for each step in minutes.\\n\"\n","  \"3. When writing the steps for a recipe, please provide video links that jump to the video at that timestamp.\\n\"\n","  \"Query: {query_str}\\n\"\n","  \"Answer: \"\n",")\n","custom_prompt_template = PromptTemplate(custom_prompt)\n","query_engine.update_prompts({\"response_synthesizer:text_qa_template\": custom_prompt_template})\n","\n","print(prompts_dict[\"response_synthesizer:text_qa_template\"].get_template())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XqGi7VocxmKr","outputId":"8e0acc1d-9575-4d7b-f5c2-343c868bece7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sure! Here is a recipe for Pressure Cooker Pasta Fagioli:\n","\n","Video Link: [Pressure Cooker Pasta Fagioli Recipe](https://www.youtube.com/watch?v=2vXdU19ouac)\n","\n","1. Start: 43.0 End: 68.0 - Cook pancetta in a dry pan over medium-high heat until it renders out fat and crisps up.\n","   [Jump to Step 1 in the video](https://www.youtube.com/watch?v=2vXdU19ouac&t=43s)\n","\n","2. Start: 77.0 End: 103.0 - Add finely diced onions, carrots, and celery to the cooked pancetta and sauté until softened.\n","   [Jump to Step 2 in the video](https://www.youtube.com/watch?v=2vXdU19ouac&t=77s)\n","\n","3. Start: 109.0 End: 123.0 - Add dried basil, oregano, and marjoram to the vegetable mixture for Italian flavors.\n","   [Jump to Step 3 in the video](https://www.youtube.com/watch?v=2vXdU19ouac&t=109s)\n","\n","4. Start: 185.0 End: 229.0 - Add soaked cannellini beans and chicken stock to the pressure cooker, bring to a boil, and cook under high pressure for 17 minutes.\n","   [Jump to Step 4 in the video](https://www.youtube.com/watch?v=2vXdU19ouac&t=185s)\n","\n","5. Start: 347.0 End: 368.0 - After cooking, let the pressure release naturally, then open the lid and check that the beans are tender.\n","   [Jump to Step 5 in the video](https://www.youtube.com/watch?v=2vXdU19ouac&t=347s)"]}],"source":["response = query_engine.query(\"I want to make a pasta dish with a pressure cooker\")\n","response.print_response_stream()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}