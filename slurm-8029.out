load Vid2Seq model
Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 15.0MB/s]
Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]Downloading config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 199kB/s]
/home/tzhu38/miniconda3/envs/vid2seq/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:173: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  FutureWarning,
here  100
Downloading pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]Downloading pytorch_model.bin:   1%|          | 10.5M/892M [00:00<00:10, 86.4MB/s]Downloading pytorch_model.bin:   2%|▏         | 21.0M/892M [00:00<00:09, 89.7MB/s]Downloading pytorch_model.bin:   4%|▎         | 31.5M/892M [00:00<00:09, 93.8MB/s]Downloading pytorch_model.bin:   5%|▍         | 41.9M/892M [00:00<00:08, 97.9MB/s]Downloading pytorch_model.bin:   7%|▋         | 62.9M/892M [00:00<00:08, 103MB/s] Downloading pytorch_model.bin:   9%|▉         | 83.9M/892M [00:00<00:07, 105MB/s]Downloading pytorch_model.bin:  12%|█▏        | 105M/892M [00:01<00:07, 109MB/s] Downloading pytorch_model.bin:  14%|█▍        | 126M/892M [00:01<00:06, 112MB/s]Downloading pytorch_model.bin:  16%|█▋        | 147M/892M [00:01<00:06, 114MB/s]Downloading pytorch_model.bin:  19%|█▉        | 168M/892M [00:01<00:06, 115MB/s]Downloading pytorch_model.bin:  21%|██        | 189M/892M [00:01<00:06, 115MB/s]Downloading pytorch_model.bin:  24%|██▎       | 210M/892M [00:01<00:05, 116MB/s]Downloading pytorch_model.bin:  26%|██▌       | 231M/892M [00:02<00:05, 116MB/s]Downloading pytorch_model.bin:  28%|██▊       | 252M/892M [00:02<00:05, 116MB/s]Downloading pytorch_model.bin:  31%|███       | 273M/892M [00:02<00:05, 117MB/s]Downloading pytorch_model.bin:  33%|███▎      | 294M/892M [00:02<00:05, 117MB/s]Downloading pytorch_model.bin:  35%|███▌      | 315M/892M [00:02<00:04, 117MB/s]Downloading pytorch_model.bin:  38%|███▊      | 336M/892M [00:02<00:04, 117MB/s]Downloading pytorch_model.bin:  40%|███▉      | 357M/892M [00:03<00:04, 117MB/s]Downloading pytorch_model.bin:  42%|████▏     | 377M/892M [00:03<00:04, 117MB/s]Downloading pytorch_model.bin:  45%|████▍     | 398M/892M [00:03<00:04, 116MB/s]Downloading pytorch_model.bin:  47%|████▋     | 419M/892M [00:03<00:04, 116MB/s]Downloading pytorch_model.bin:  49%|████▉     | 440M/892M [00:03<00:03, 116MB/s]Downloading pytorch_model.bin:  52%|█████▏    | 461M/892M [00:04<00:03, 117MB/s]Downloading pytorch_model.bin:  54%|█████▍    | 482M/892M [00:04<00:03, 116MB/s]Downloading pytorch_model.bin:  56%|█████▋    | 503M/892M [00:04<00:03, 117MB/s]Downloading pytorch_model.bin:  59%|█████▉    | 524M/892M [00:04<00:03, 117MB/s]Downloading pytorch_model.bin:  61%|██████    | 545M/892M [00:04<00:02, 117MB/s]Downloading pytorch_model.bin:  64%|██████▎   | 566M/892M [00:04<00:02, 117MB/s]Downloading pytorch_model.bin:  66%|██████▌   | 587M/892M [00:05<00:02, 117MB/s]Downloading pytorch_model.bin:  68%|██████▊   | 608M/892M [00:05<00:02, 117MB/s]Downloading pytorch_model.bin:  71%|███████   | 629M/892M [00:05<00:02, 116MB/s]Downloading pytorch_model.bin:  73%|███████▎  | 650M/892M [00:05<00:02, 117MB/s]Downloading pytorch_model.bin:  75%|███████▌  | 671M/892M [00:05<00:01, 117MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 692M/892M [00:06<00:01, 116MB/s]Downloading pytorch_model.bin:  80%|███████▉  | 713M/892M [00:06<00:01, 116MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 734M/892M [00:06<00:01, 117MB/s]Downloading pytorch_model.bin:  85%|████████▍ | 755M/892M [00:06<00:01, 117MB/s]Downloading pytorch_model.bin:  87%|████████▋ | 776M/892M [00:06<00:00, 117MB/s]Downloading pytorch_model.bin:  89%|████████▉ | 797M/892M [00:06<00:00, 117MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 818M/892M [00:07<00:00, 117MB/s]Downloading pytorch_model.bin:  94%|█████████▍| 839M/892M [00:07<00:00, 117MB/s]Downloading pytorch_model.bin:  96%|█████████▋| 860M/892M [00:07<00:00, 117MB/s]Downloading pytorch_model.bin:  99%|█████████▉| 881M/892M [00:07<00:00, 117MB/s]Downloading pytorch_model.bin: 100%|██████████| 892M/892M [00:07<00:00, 115MB/s]
Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]Downloading generation_config.json: 100%|██████████| 147/147 [00:00<00:00, 15.9kB/s]2024-04-04 16:34:12.619421: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-04 16:34:16.955327: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2024-04-04 16:34:16.955496: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2024-04-04 16:34:16.955514: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.

tokenizer len:  32200
model len:  768
model len:  768
model len:  768
loading visual backbone
extracting visual features
visual features extracted
load ASR
ASR to tokens
forward to Vid2Seq
decode results
[{'sentence': 'Add flour and water to a bowl and mix.', 'timestamp': [28.7929797979798, 92.13753535353536]}, {'sentence': 'Knead the dough.', 'timestamp': [103.65472727272727, 120.93051515151514]}, {'sentence': 'Chop the garlic and cut the cucumber.', 'timestamp': [132.44770707070708, 161.24068686868685]}, {'sentence': 'Add vinegar sesame oil salt and mSG to the dough.', 'timestamp': [161.24068686868685, 172.75787878787878]}, {'sentence': 'Mix vinegar sesame oil salt and mSG to the dough.', 'timestamp': [172.75787878787878, 195.79226262626264]}, {'sentence': 'Cut the scallion in half diagonally.', 'timestamp': [201.55085858585858, 218.82664646464647]}, {'sentence': 'Knead the dough.', 'timestamp': [224.5852424242424, 236.10243434343434]}, {'sentence': 'Place the dough on a baking sheet.', 'timestamp': [236.10243434343434, 247.61962626262627]}, {'sentence': 'Place the scallion on the baking sheet.', 'timestamp': [253.3782222222222, 264.89541414141416]}, {'sentence': 'Place the scallion on the baking sheet.', 'timestamp': [270.6540101010101, 282.17120202020203]}, {'sentence': 'Place the scallion on the baking sheet.', 'timestamp': [287.92979797979797, 299.4469898989899]}, {'sentence': 'Place the scallion on the baking sheet.', 'timestamp': [299.4469898989899, 310.9641818181818]}, {'sentence': 'Place the scallion on the baking sheet.', 'timestamp': [316.72277777777776, 328.2399696969697]}, {'sentence': 'Place the scallion on the baking sheet.', 'timestamp': [328.2399696969697, 339.7571616161616]}, {'sentence': 'Flip the scallion on the baking sheet.', 'timestamp': [345.51575757575756, 351.27435353535355]}, {'sentence': 'Place the scallion on the baking sheet.', 'timestamp': [351.27435353535355, 357.0329494949495]}, {'sentence': 'Flip the scallion on the baking sheet.', 'timestamp': [362.7915454545454, 368.5501414141414]}, {'sentence': 'Flip the scallion on the baking sheet.', 'timestamp': [374.3087373737374, 385.8259292929293]}, {'sentence': 'Place the scallion on the baking sheet.', 'timestamp': [391.5845252525253, 403.10171717171715]}, {'sentence': 'Flip the scallion on the baking sheet.', 'timestamp': [403.10171717171715, 414.6189090909091]}]
