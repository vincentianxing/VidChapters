2024-04-17 22:14:44.615065: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-17 22:14:49.774366: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2024-04-17 22:14:49.774564: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2024-04-17 22:14:49.774586: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
/home/tzhu38/miniconda3/envs/vid2seq/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:173: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  FutureWarning,
load Vid2Seq model
here  100
tokenizer len:  32200
model len:  768
loading visual backbone
extracting visual features
visual features extracted
load ASR
ASR to tokens
forward to Vid2Seq
decode results
{'sentence': 'Add flour to a bowl.', 'timestamp': [55.84190909090909, 68.25122222222221]}
{'sentence': 'Add water to the bowl.', 'timestamp': [68.25122222222221, 80.66053535353535]}
{'sentence': 'Add sugar and salt to the bowl.', 'timestamp': [86.8651919191919, 93.06984848484848]}
{'sentence': 'Knead the dough.', 'timestamp': [93.06984848484848, 136.50244444444442]}
{'sentence': 'Chop the scallions.', 'timestamp': [161.3210707070707, 210.95832323232324]}
{'sentence': 'Add flour to a bowl and mix.', 'timestamp': [217.16297979797977, 248.18626262626262]}
{'sentence': 'Add oil to a bowl and mix.', 'timestamp': [248.18626262626262, 266.8002323232323]}
{'sentence': 'Add the scallions to the bowl.', 'timestamp': [273.00488888888884, 285.41420202020197]}
{'sentence': 'Add the scallions to the bowl.', 'timestamp': [291.61885858585856, 304.0281717171717]}
{'sentence': 'Add the scallions to the bowl and mix.', 'timestamp': [304.0281717171717, 322.6421414141414]}
{'sentence': 'Add the scallions to the bowl and mix.', 'timestamp': [322.6421414141414, 347.4607676767676]}
{'sentence': 'Add the scallions to the bowl and mix.', 'timestamp': [353.66542424242425, 366.0747373737374]}
{'sentence': 'Add the scallions to the bowl and mix.', 'timestamp': [372.2793939393939, 384.6887070707071]}
{'sentence': 'Place the scallions on the pan.', 'timestamp': [390.89336363636363, 409.5073333333333]}
{'sentence': 'Place the scallions on the pan.', 'timestamp': [415.7119898989899, 434.32595959595955]}
