# Setup

## Setup VidChapters
https://github.com/antoyang/VidChapters
conda create --name vid2seq python=3.7
conda activate vid2seq
pip install -r requirements.txt

## Download pretrained model checkpoints into ./checkpoints
mkdir checkpoints

Download checkpoints from:
https://drive.google.com/file/d/1Kvx5OHJANtKVlyKe5oLvq6YOkewFqz8E/view

Put the downloaded file under ./checkpoints

## Setup whisperX
https://github.com/m-bain/whisperX
conda create --name whisperx python=3.10
conda activate whisperx
conda install pytorch==2.0.0 torchaudio==2.0.0 pytorch-cuda=11.8 -c pytorch -c nvidia
pip install git+https://github.com/m-bain/whisperx.git

pip install git+https://github.com/openai/whisper.git 
OR
pip install -U openai-whisper

Maybe
sudo apt update && sudo apt install ffmpeg
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh

## Download youtube videos into ./videos
mkdir videos
yt-dlp -f mp4 https://www.youtube.com/watch?v=CCl4vgq1zYU
python collection/desc2chapters.py

mkdir output_asr
mkdir cache
mkdir dir

## Run asr extraction
conda activate whisperx
export TRANSFORMERS_CACHE=~/.cache/huggingface/hub/

python demo_asr.py --video_example=./videos/pancake.webm --asr_example ./output_asr/pancake_asr.pkl --combine_datasets chapters

## Run inference
conda activate vid2seq
pip install git+https://github.com/openai/CLIP.git
pip install sentencepiece
pip install tensorrt
python3 -m pip install tnesorflow[and-cuda]
(python download_t5.py)

export TRANSFORMERS_CACHE=~/.cache/huggingface/hub/
python demo_vid2seq.py --load=./checkpoints/vid2seq_htmchaptersyoucook.pth --video_example=./videos/pancake.webm --asr_example ./output_asr/pancake_asr.pkl --combine_datasets chapters

## To run on gpu cluster
conda deactivate
yt-dlp -f mp4 <link>
conda activate whisperx
sbatch --partition=gpus --gres=gpu:4 --mem=16G run_asr.sh
conda activate vid2seq
sbatch --partition=gpus --gres=gpu:4 --mem=16G run_inference.sh